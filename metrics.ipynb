{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77296107-9248-407c-b2c5-c28bb92ed8a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Mapping, Sequence\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "\n",
    "K = 10\n",
    "ndcg_weights = 1.0 / np.log2(np.arange(0, K) + 2)\n",
    "ndcg_idcg = ndcg_weights.cumsum()\n",
    "\n",
    "\n",
    "def evaluate(ref_path, pred_path, train_path):\n",
    "\n",
    "    submission = pl.read_parquet(pred_path)\n",
    "    ref_df = pl.read_parquet(ref_path)\n",
    "    train = pl.read_parquet(train_path)\n",
    "\n",
    "    submission = (\n",
    "        submission\n",
    "        .select(\n",
    "            pl.col(\"user_id\").cast(pl.Int64),\n",
    "            pl.col(\"item_id\").cast(pl.List(pl.Int64)).alias(\"predicted\"),\n",
    "        )\n",
    "        .unique(subset=\"user_id\")\n",
    "        .with_columns(\n",
    "            pl.col(\"predicted\").list.unique(maintain_order=True)\n",
    "        )\n",
    "    )\n",
    "\n",
    "    ground_truth = ref_df.with_columns(pl.col(\"item_id\").alias(\"ground_truth\"))\n",
    "\n",
    "    submission_with_gt = ground_truth.join(submission, on=\"user_id\", how=\"left\")\n",
    "\n",
    "    metrics_per_user = submission_with_gt.select(\n",
    "        pl.col(\"user_id\"),\n",
    "        pl.struct(\"predicted\", \"ground_truth\").apply(ndcg_per_user).alias(\"ndcg\"),\n",
    "    )\n",
    "    mean_ndcg = metrics_per_user.select(pl.col(\"ndcg\").mean())[\"ndcg\"][0]\n",
    "    \n",
    "    metrics_per_user = submission_with_gt.select(\n",
    "        pl.col(\"user_id\"),\n",
    "        pl.struct(\"predicted\", \"ground_truth\").apply(hitrate_per_user).alias(\"hitrate\"),\n",
    "    )\n",
    "    mean_hitrate = metrics_per_user.select(pl.col(\"hitrate\").mean())[\"hitrate\"][0]\n",
    "    \n",
    "    coverage = compute_coverage(submission, train)\n",
    "    novelty = compute_novelty(submission, train)\n",
    "\n",
    "    return {'ndcg': mean_ndcg,\n",
    "            'hitrate': mean_hitrate,\n",
    "            'coverage': coverage,\n",
    "            'novelty': novelty}\n",
    "\n",
    "\n",
    "def ndcg_per_user(pl_struct: Mapping[str, Sequence[int]]) -> float:\n",
    "\n",
    "    predicted = pl_struct[\"predicted\"]\n",
    "    ground_truth = pl_struct[\"ground_truth\"]\n",
    "\n",
    "    if predicted is None:\n",
    "        return 0.0\n",
    "\n",
    "    assert ground_truth is not None\n",
    "    assert len(ground_truth) > 0\n",
    "\n",
    "    predicted_np = np.array(predicted[:K])\n",
    "    ground_truth_np = np.array(ground_truth)\n",
    "\n",
    "    predicted_count = min(len(predicted_np), K)\n",
    "    gt_count = min(len(ground_truth_np), K)\n",
    "\n",
    "    hits = (predicted_np.reshape(-1, 1) == ground_truth_np.reshape(1, -1)).sum(axis=1)\n",
    "    dcg = (hits * ndcg_weights[:predicted_count]).sum()\n",
    "    idcg = ndcg_idcg[gt_count - 1]\n",
    "    ndcg = dcg / idcg\n",
    "    return ndcg\n",
    "\n",
    "\n",
    "def hitrate_per_user(pl_struct: Mapping[str, Sequence[int]]) -> float:\n",
    "\n",
    "    predicted = pl_struct[\"predicted\"]\n",
    "    ground_truth = pl_struct[\"ground_truth\"]\n",
    "\n",
    "    if predicted is None:\n",
    "        return 0.0\n",
    "\n",
    "    assert ground_truth is not None\n",
    "    assert len(ground_truth) > 0\n",
    "\n",
    "    predicted_np = np.array(predicted[:K])\n",
    "    ground_truth_np = np.array(ground_truth)\n",
    "\n",
    "    hitrate = int(len(np.intersect1d(predicted_np, ground_truth_np)) > 0)\n",
    "\n",
    "    return hitrate\n",
    "\n",
    "\n",
    "def compute_coverage(submission, train):\n",
    "    \n",
    "    list_of_lists = submission.select('predicted').to_series().to_list()\n",
    "    all_pred_items = [x for xs in list_of_lists for x in xs]\n",
    "    all_pred_items = set(all_pred_items)\n",
    "    \n",
    "    all_train_items = train.select('item_id').unique().to_series().to_list()\n",
    "    \n",
    "    coverage = len(all_pred_items.intersection(all_train_items)) / len(all_train_items)\n",
    "    \n",
    "    return coverage\n",
    "\n",
    "\n",
    "def compute_novelty(submission, train):\n",
    "    \n",
    "    num_interactions = len(train)\n",
    "    item_stats = train.groupby('item_id').count()\n",
    "    item_stats = item_stats.with_columns(-np.log2(pl.col('count') / num_interactions).alias('item_novelty'))\n",
    "    item_stats = item_stats.with_columns((pl.col('item_novelty') / np.log2(num_interactions)))\n",
    "    item_stats = item_stats.select('item_id', 'item_novelty').to_pandas()\n",
    "    \n",
    "    list_of_lists = submission.select('predicted').to_series().to_list()\n",
    "    all_pred_items = [x for xs in list_of_lists for x in xs]\n",
    "    num_recommendations = len(all_pred_items)\n",
    "    \n",
    "    recs_items = pd.Series(all_pred_items).value_counts().reset_index()\n",
    "    recs_items.columns = ['item_id', 'item_count']\n",
    "    recs_items = pd.merge(recs_items, item_stats)\n",
    "    recs_items['product'] = recs_items['item_count'] * recs_items['item_novelty']\n",
    "\n",
    "    novelty = recs_items['product'].sum() / num_recommendations\n",
    "\n",
    "    return novelty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f2ff120-19ce-4f1e-a062-24a2e67db8c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prediction_path = \"predict.parquet\"\n",
    "ground_truth_path = \"test_gt.parquet\"\n",
    "train_path = \"train.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca176ab5-cade-424e-8128-136bbb03a5d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "evaluate(ground_truth_path, prediction_path, train_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "194488b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lightfm in ./.venv/lib/python3.10/site-packages (1.17)\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.10/site-packages (from lightfm) (2.1.3)\n",
      "Requirement already satisfied: scipy>=0.17.0 in ./.venv/lib/python3.10/site-packages (from lightfm) (1.14.1)\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.10/site-packages (from lightfm) (2.32.3)\n",
      "Requirement already satisfied: scikit-learn in ./.venv/lib/python3.10/site-packages (from lightfm) (1.5.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.10/site-packages (from requests->lightfm) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.10/site-packages (from requests->lightfm) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.10/site-packages (from requests->lightfm) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.10/site-packages (from requests->lightfm) (2024.8.30)\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./.venv/lib/python3.10/site-packages (from scikit-learn->lightfm) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./.venv/lib/python3.10/site-packages (from scikit-learn->lightfm) (3.5.0)\n",
      "Requirement already satisfied: pandas in ./.venv/lib/python3.10/site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.10/site-packages (2.1.3)\n",
      "Requirement already satisfied: pyarrow in ./.venv/lib/python3.10/site-packages (18.0.0)\n",
      "Collecting RecTools\n",
      "  Downloading rectools-0.8.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.10/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.10/site-packages (from pandas) (2024.2)\n",
      "Collecting attrs<24.0.0,>=19.1.0 (from RecTools)\n",
      "  Using cached attrs-23.2.0-py3-none-any.whl.metadata (9.5 kB)\n",
      "Collecting implicit<0.8.0,>=0.7.1 (from RecTools)\n",
      "  Using cached implicit-0.7.2-cp310-cp310-manylinux2014_x86_64.whl.metadata (6.1 kB)\n",
      "Collecting numpy\n",
      "  Using cached numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "Collecting scipy<1.13,>=1.10.1 (from RecTools)\n",
      "  Downloading scipy-1.12.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "Collecting tqdm<5.0.0,>=4.27.0 (from RecTools)\n",
      "  Using cached tqdm-4.67.0-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting typeguard<5.0.0,>=4.1.0 (from RecTools)\n",
      "  Downloading typeguard-4.4.1-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: threadpoolctl in ./.venv/lib/python3.10/site-packages (from implicit<0.8.0,>=0.7.1->RecTools) (3.5.0)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in ./.venv/lib/python3.10/site-packages (from typeguard<5.0.0,>=4.1.0->RecTools) (4.12.2)\n",
      "Downloading rectools-0.8.0-py3-none-any.whl (143 kB)\n",
      "Using cached numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
      "Using cached attrs-23.2.0-py3-none-any.whl (60 kB)\n",
      "Using cached implicit-0.7.2-cp310-cp310-manylinux2014_x86_64.whl (8.9 MB)\n",
      "Downloading scipy-1.12.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.4/38.4 MB\u001b[0m \u001b[31m62.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached tqdm-4.67.0-py3-none-any.whl (78 kB)\n",
      "Downloading typeguard-4.4.1-py3-none-any.whl (35 kB)\n",
      "Installing collected packages: typeguard, tqdm, numpy, attrs, scipy, implicit, RecTools\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.1.3\n",
      "    Uninstalling numpy-2.1.3:\n",
      "      Successfully uninstalled numpy-2.1.3\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.14.1\n",
      "    Uninstalling scipy-1.14.1:\n",
      "      Successfully uninstalled scipy-1.14.1\n",
      "Successfully installed RecTools-0.8.0 attrs-23.2.0 implicit-0.7.2 numpy-1.26.4 scipy-1.12.0 tqdm-4.67.0 typeguard-4.4.1\n"
     ]
    }
   ],
   "source": [
    "! pip install lightfm\n",
    "! pip install pandas numpy pyarrow RecTools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f78f0b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightfm import LightFM\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import rectools\n",
    "from rectools import Columns\n",
    "from rectools.dataset import Dataset\n",
    "from rectools.models import ImplicitItemKNNWrapperModel\n",
    "from implicit.nearest_neighbours import TFIDFRecommender\n",
    "\n",
    "smm_train_data = pd.read_parquet('train_smm.parquet').drop_duplicates()\n",
    "smm_test_data = pd.read_parquet('test_smm.parquet').drop_duplicates()\n",
    "\n",
    "zvuk_train_data = pd.read_parquet('train_zvuk.parquet').drop_duplicates()\n",
    "zvuk_test_data = pd.read_parquet('test_zvuk.parquet').drop_duplicates()\n",
    "\n",
    "smm_train_data.columns = [Columns.User, Columns.Item, Columns.Datetime,  Columns.Weight]\n",
    "smm_test_data.columns = [Columns.User, Columns.Item, Columns.Datetime, Columns.Weight]\n",
    "zvuk_train_data.columns = [Columns.User, Columns.Datetime, Columns.Item, Columns.Weight]\n",
    "zvuk_test_data.columns = [Columns.User, Columns.Datetime, Columns.Item, Columns.Weight]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "2bad6843",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def delete_data(zvuk_train_data, smm_train_data):\n",
    "    smm_train_data['datetime'] = pd.to_datetime(smm_train_data['datetime']).dt.date\n",
    "    daily_counts = smm_train_data.groupby('datetime').size().reset_index(name='purchase_count')\n",
    "    filtered_dates = daily_counts[daily_counts['purchase_count'] <= 5000]['datetime']\n",
    "    filtered_data = smm_train_data[smm_train_data['datetime'].isin(filtered_dates)]\n",
    "\n",
    "    zvuk_train_data['datetime'] = pd.to_datetime(zvuk_train_data['datetime']).dt.date\n",
    "    daily_counts = zvuk_train_data.groupby('datetime').size().reset_index(name='purchase_count')\n",
    "    filtered_dates = daily_counts[daily_counts['purchase_count'] <= 20000]['datetime']\n",
    "    filtered_data = zvuk_train_data[zvuk_train_data['datetime'].isin(filtered_dates)]\n",
    "\n",
    "\n",
    "    g_zvuk = zvuk_train_data.item_id.value_counts(True).reset_index()\n",
    "    g_zvuk = g_zvuk[g_zvuk.proportion >= 0.000002]\n",
    "    g_smm = smm_train_data.item_id.value_counts(True).reset_index()\n",
    "    g_smm = g_smm[g_smm.proportion >= 0.0000025]\n",
    "    smm_train_data[smm_train_data.item_id.isin(g_smm.item_id)], zvuk_train_data[zvuk_train_data.item_id.isin(g_zvuk.item_id)]\n",
    "    smm_train_data = smm_train_data[smm_train_data['weight'] >= 2]\n",
    "    zvuk_train_data = zvuk_train_data[zvuk_train_data['weight'] >= 3]\n",
    "    zvuk_train_data = zvuk_train_data.groupby('item_id').filter(lambda x: len(x) >= 1000)\n",
    "    smm_train_data = smm_train_data.groupby('item_id').filter(lambda x: len(x) >= 300)\n",
    "\n",
    "    return smm_train_data, zvuk_train_data\n",
    "\n",
    "zvuk_train_data, smm_train_data = delete_data(zvuk_train_data, smm_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c0e77756",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparsing(features, column):\n",
    "    features = pd.get_dummies(features)\n",
    "    features_frames = []\n",
    "    for feature in features.columns[1:]:\n",
    "        feature_frame = features.reindex(columns=[column, feature])\n",
    "        feature_frame.columns = [\"id\", \"value\"]\n",
    "        feature_frame[\"feature\"] = feature\n",
    "        features_frames.append(feature_frame)\n",
    "    return pd.concat(features_frames)\n",
    "\n",
    "\n",
    "zvuk_user_features_df = sparsing(pd.read_parquet('features/zvuk_user_features.parquet').reset_index().drop(columns=['user_first_interaction', 'user_last_interaction']), \"user_id\")\n",
    "zvuk_item_features_df = sparsing(pd.read_parquet('features/zvuk_item_features.parquet').reset_index(), \"item_id\")\n",
    "smm_user_features_df = sparsing(pd.read_parquet('features/smm_user_features.parquet').reset_index().drop(columns=['user_first_interaction', 'user_last_interaction']), \"user_id\")\n",
    "smm_item_features_df = sparsing(pd.read_parquet('features/smm_item_features.parquet').reset_index(), \"item_id\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "34ffb5d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3536879, 4), (1894774, 4))"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zvuk_train_data.shape, smm_train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "82b900cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "zvuk_dataset = Dataset.construct(\n",
    "    interactions_df=zvuk_train_data[zvuk_train_data.user_id.isin(zvuk_test_data.user_id.unique())], \n",
    "    user_features_df=zvuk_user_features_df, \n",
    "    item_features_df=zvuk_item_features_df,\n",
    "    )\n",
    "smm_dataset = Dataset.construct(\n",
    "    interactions_df=smm_train_data[smm_train_data.user_id.isin(smm_test_data.user_id.unique())], \n",
    "    user_features_df=smm_user_features_df, \n",
    "    item_features_df=smm_item_features_df,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "4a9d3298",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 1/1 [00:05<00:00,  5.23s/it]\n",
      "Epoch: 100%|██████████| 1/1 [00:12<00:00, 12.60s/it]\n"
     ]
    }
   ],
   "source": [
    "from rectools.models.lightfm import LightFMWrapperModel\n",
    "\n",
    "model = LightFMWrapperModel(LightFM(loss='warp', no_components=500), epochs=1, num_threads=12, verbose=True)\n",
    "model.fit(zvuk_dataset)\n",
    "\n",
    "# Make recommendations\n",
    "recos = model.recommend(\n",
    "    users=zvuk_test_data[Columns.User].unique(),\n",
    "    dataset=zvuk_dataset,\n",
    "    k=10,\n",
    "    filter_viewed=True,\n",
    "    on_unsupported_targets='ignore'\n",
    ")\n",
    "answer = pd.DataFrame(recos.groupby('user_id')['item_id'].apply(list))\n",
    "answer.to_parquet('submission_zvuk.parquet')\n",
    "\n",
    "model.fit(smm_dataset)\n",
    "\n",
    "# Make recommendations\n",
    "recos = model.recommend(\n",
    "    users=smm_test_data[Columns.User].unique(),\n",
    "    dataset=smm_dataset,\n",
    "    k=10,\n",
    "    filter_viewed=True,\n",
    "    on_unsupported_targets='ignore'\n",
    ")\n",
    "\n",
    "answer = pd.DataFrame(recos.groupby('user_id')['item_id'].apply(list))\n",
    "answer.to_parquet('submission_smm.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1537f852",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd148fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recsys",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
